{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "names = [\"condensed_2010.json\",\"condensed_2011.json\",\"condensed_2012.json\",\"condensed_2013.json\",\n",
    "         \"condensed_2014.json\", \"condensed_2015.json\",\"condensed_2016.json\",\"condensed_2017.json\",\"condensed_2018.json\",\n",
    "         \"trump2019_26_03.json\"]\n",
    "\n",
    "data = pd.read_json(\"condensed_2009.json\")\n",
    "for element in names:\n",
    "    data1 = pd.read_json(element)\n",
    "    print(len(data1),element)\n",
    "    data = data.append(data1,sort=True)\n",
    "data = data.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#pd.to_datetime(stamps, format=\"%Y-%m-%d %H:%M:%S\").sort_values()\n",
    "\n",
    "data['created_at'] = pd.to_datetime(data.created_at, format=\"%Y-%m-%d %H:%M:%S\")\n",
    "data.sort_values(by=['created_at'], inplace=True, ascending=True)\n",
    "data.reset_index(inplace=True,drop=True)\n",
    "data.to_json(\"orderedTwitterArchive.json\")\n",
    "data.to_csv(\"orderedTwitterArchive.csv\")\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_json(\"orderedTwitterArchive.json\")\n",
    "data['created_at'] = pd.to_datetime(data.created_at, format=\"%Y-%m-%d %H:%M:%S\")\n",
    "data.sort_values(by=['created_at'], inplace=True, ascending=True)\n",
    "data.reset_index(inplace=True,drop=True)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (25, 10))\n",
    "ax = sns.lineplot(x=data.index, y=\"favorite_count\", data=data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (25, 10))\n",
    "ax = sns.lineplot(x=data.index, y=\"retweet_count\", data=data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"text\"][2005]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check tags\n",
    "ats = dict()\n",
    "for idx,row in data.iterrows():\n",
    "    l = re.findall(\"^@.[a-zA-Z0-9]*\",row[\"text\"])\n",
    "    for el in l:\n",
    "        if el not in ats:\n",
    "            ats[el] = 1\n",
    "        else:\n",
    "            ats[el] = ats[el]+1\n",
    "print(len(ats))\n",
    "ats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = pd.Series(ats)\n",
    "t = []\n",
    "for el in s.index:\n",
    "    if s[el] <= 2:\n",
    "        t.append(el)\n",
    "len(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for el in t:\n",
    "    s.drop(el,inplace=True)\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sta sui 5 minuti, aggiunge una colonna per ogni tag con più di tot apparizioni (per migliorare il clustering?)\n",
    "for tag in s.index:\n",
    "    column = []\n",
    "    for idx,row in data.iterrows():\n",
    "        if tag in row[\"text\"].split():\n",
    "            column.append(1)\n",
    "        else:\n",
    "            column.append(0)\n",
    "    data[tag] = pd.Series(column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check hastags\n",
    "hashtags = dict()\n",
    "for idx,row in data.iterrows():\n",
    "    l = re.findall(\"^#[A-z]*\",row[\"text\"])\n",
    "    for el in l:\n",
    "        if el not in ats:\n",
    "            hashtags[el] = 1\n",
    "        else:\n",
    "            hashtags[el] = hashtags[el]+1\n",
    "hashtags\n",
    "#usa pochissimo gli hashtag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove hashtags, useless\n",
    "for idx,row in data.iterrows():\n",
    "    re.sub(\"^#[A-z]*\",\"\",row[\"text\"])\n",
    "#usa pochissimo gli hashtag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "genereate new column with modded text, remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "data[\"modded_text\"] = data[\"text\"].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove punctuation\n",
    "data['modded_text'] = data[\"modded_text\"].str.replace(\"[^\\w\\s]\",'')\n",
    "\n",
    "#stemming\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "stemmedDataset = pd.DataFrame()\n",
    "data1 = data\n",
    "\n",
    "stemDescriptions = []\n",
    "for idx, row in data1.iterrows():\n",
    "    desc = \"\"\n",
    "    for el in row[\"text\"].split():\n",
    "        desc = desc + stemmer.stem(el) + \" \"\n",
    "    stemDescriptions.append(desc)\n",
    "    \n",
    "data1[\"modded_text\"] = stemDescriptions\n",
    "\n",
    "\n",
    "#unique words\n",
    "unique = pd.Series(' '.join(data1['modded_text']).split()).value_counts()\n",
    "print(len(unique))\n",
    "#devi rimuovere le cose con valore meno di tot e più di tot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1[\"modded_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unfreq = []\n",
    "for el in unique.index:\n",
    "    if unique[el] <= 5:\n",
    "        unfreq.append(el)\n",
    "len(unfreq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# idee\n",
    "\n",
    "NLP:\n",
    "    - pos tagging per contare numero di aggettivi \n",
    "    - qualcosa per vedere se aggettivi sono positivi o negativi (direttamente sentiment analysis)?\n",
    "    - rimuovere gli @ vs tenere solo i più frequenti \n",
    "    - rimuovere i link\n",
    "    - pulire gli hashtag? \n",
    "    - word2vec distance per avere simili tweet assieme nello spazio vettoriale?\n",
    "    - ridurre parole presenti? \n",
    "    - tfidf?\n",
    "    \n",
    "Processo:\n",
    "    - clustering interno all'anno\n",
    "    - clustering generale \n",
    "    - clustering con/senza sentiment meaning"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
