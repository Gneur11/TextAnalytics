{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "import gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# idee\n",
    "\n",
    "NLP:\n",
    "    - pos tagging per contare numero di aggettivi \n",
    "    - qualcosa per vedere se aggettivi sono positivi o negativi (direttamente sentiment analysis)?\n",
    "    - rimuovere gli @ vs tenere solo i più frequenti \n",
    "    - rimuovere i link\n",
    "    - pulire gli hashtag? \n",
    "    - word2vec distance per avere simili tweet assieme nello spazio vettoriale?\n",
    "    - ridurre parole presenti? \n",
    "    - tfidf?\n",
    "    \n",
    "Processo:\n",
    "    - clustering interno all'anno\n",
    "    - clustering generale \n",
    "    - clustering con/senza sentiment meaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_json(\"dataWithTags.json\")\n",
    "data['created_at'] = pd.to_datetime(data.created_at, format=\"%Y-%m-%d %H:%M:%S\")\n",
    "data.sort_values(by=['created_at'], inplace=True, ascending=True)\n",
    "data.reset_index(inplace=True,drop=True)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop([\"in_reply_to_user_id_str\",\"id_str\",\"source\"], inplace=True,axis=1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggiungi dati per il tempo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exploit time\n",
    "data['month'] = [d.month for d in data['created_at']]\n",
    "data[\"year\"] = [d.year for d in data[\"created_at\"]]\n",
    "data[\"hour\"] = [d.hour for d in data[\"created_at\"]]\n",
    "data[\"week_year\"] = [d.weekofyear for d in data[\"created_at\"]]\n",
    "data[\"date\"] = [d.date() for d in data[\"created_at\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = data.groupby([\"year\"])[\"year\"].count()\n",
    "plt.plot(temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "annuncio campagna elettorale giugno 2015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = pd.DataFrame({'count' : data.groupby( [ \"year\", \"month\"] ).size()}).reset_index()\n",
    "#temp\n",
    "#plotta nel corso dei mesi il numero di tweet per anni, devi prendere i dati per ogni anno e farli di lunghezza uguale come array e poi plotti "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = []\n",
    "for year in temp[\"year\"].unique().tolist():\n",
    "    l1 = [0] * 12\n",
    "    t = temp[temp[\"year\"] == year]\n",
    "    for idx,row in t.iterrows():\n",
    "        l1[row[\"month\"]-1] = row[\"count\"]\n",
    "    l.append(l1)\n",
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leg = temp[\"year\"].unique().tolist()\n",
    "fig, ax = plt.subplots(figsize = (15, 10))\n",
    "months = [x for x in range(0,12)]\n",
    "for i in range(0,len(l)):\n",
    "    plt.plot(months, l[i],label = leg[i])\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(\"created_at\", inplace=True,axis=1)\n",
    "data.drop(\"is_retweet\", inplace=True,axis=1)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# trasforma in label i valori delle date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "#trasforma la data in una label\n",
    "labeller = LabelEncoder()\n",
    "data[\"date\"] = labeller.fit_transform(data[\"date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmedTweets = []\n",
    "\n",
    "for idx, row in data.iterrows():\n",
    "    test = row[\"modded_text\"].split()\n",
    "    desc = \"\"\n",
    "    for el in test:\n",
    "        desc = desc + \" \" + stemmer.stem(el)\n",
    "    stemmedTweets.append(desc)\n",
    "temp = pd.Series(stemmedTweets)\n",
    "data[\"stemmed\"] = temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove stopwords\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "data['stemmed'] = data['stemmed'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmed_words = pd.Series(' '.join(data['stemmed']).split()).value_counts()\n",
    "print(len(stemmed_words))\n",
    "stemmed_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmed_words1 = stemmed_words[stemmed_words > 10]\n",
    "stemmed_words1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_stemmed = pd.Series(' '.join(data['modded_text']).split()).value_counts()\n",
    "print(len(non_stemmed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "import scipy.sparse as sp\n",
    "\n",
    "data1 = data.drop([\"text\",\"modded_text\"],axis=1)\n",
    "data1 = data[data[\"year\"] == 2018]\n",
    "data2 = data1[[\"stemmed\",\"date\"]]\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "X = vectorizer.fit_transform(data2[\"stemmed\"])\n",
    "\n",
    "for i, col in enumerate(vectorizer.get_feature_names()):\n",
    "    data2[col] = pd.SparseSeries(X[:, i].toarray().ravel(), fill_value=0)\n",
    "\n",
    "\n",
    "#df1 = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names())   \n",
    "#df1\n",
    "data2\n",
    "\n",
    "\n",
    "#true_k = 100\n",
    "#model = KMeans(n_clusters=true_k, init='k-means++', max_iter=100, n_init=1)\n",
    "#model.fit(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.inertia_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Top terms per cluster:\")\n",
    "order_centroids = model.cluster_centers_.argsort()[:, ::-1]\n",
    "terms = vectorizer.get_feature_names()\n",
    "for i in range(true_k):\n",
    "    print(\"Cluster %d:\" % i),\n",
    "    for ind in order_centroids[i, :10]:\n",
    "        print(' %s' % terms[ind]),\n",
    "    print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2vec embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tentativo di clustering su feature demmerda (inutile al chezzo ma vabe il codice è pronto)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data.drop(['created_at', 'text',\"modded_text\",\"id_str\",\"is_retweet\",\"in_reply_to_user_id_str\",\"source\"], axis=1,inplace=True) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "x = data.values #returns a numpy array\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "x_scaled = min_max_scaler.fit_transform(x)\n",
    "df = pd.DataFrame(x_scaled)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "kmeans = KMeans(n_clusters=2).fit(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sse_list = list()\n",
    "max_k = 15\n",
    "#plt.figure((12,6))\n",
    "for k in range(2,max_k):\n",
    "    kmeans = KMeans(init='k-means++', n_clusters=k, n_init=10, max_iter=100)\n",
    "    kmeans.fit(df)\n",
    "    sse = kmeans.inertia_\n",
    "    sse_list.append(sse)\n",
    "plt.plot(range(2,max_k), sse_list)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "kmeans = KMeans(n_clusters=40).fit(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hist, bins = np.histogram(kmeans.labels_, bins=range(0, len(set(kmeans.labels_)) + 1))\n",
    "print('centers',kmeans.cluster_centers_)\n",
    "print('labels', dict(zip(bins, hist)))\n",
    "print('sse', kmeans.inertia_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
